\chapter{Requirements and tools}
\label{ch:requirements_and_tools}

This chapter details the functional and non-functional requirements of the "Smart Pool Assistant" and presents the technological stack selected for implementation. It also describes the system's use cases and the methodology adopted for the design and development process.

\section{Functional and non-functional requirements}

The system requirements were defined to address the problems identified in the previous chapter, specifically the "Ghost Ball" visualization and the biomechanical analysis of the stroke.

\subsection{Functional Requirements}
The functional requirements define the specific behaviors and functions the system must support. They are categorized by module:

\textbf{Vision Module (Desktop):}
\begin{itemize}
    \item \textbf{FR-01 Video Acquisition:} The system must capture a real-time video stream from a webcam positioned above the billiard table.
    \item \textbf{FR-02 Object Detection:} The system must detect and classify the cue ball and object balls using a Convolutional Neural Network (YOLO).
    \item \textbf{FR-03 Pose Estimation:} The system must identify keypoints of the cue stick (tip and handle) to determine the aiming vector.
    \item \textbf{FR-04 Trajectory Prediction:} The system must calculate the "Ghost Ball" position and predict the trajectories of the cue ball and target ball based on geometric rules.
    \item \textbf{FR-05 AR Visualization:} The system must overlay the predicted paths and the Ghost Ball indicator onto the video feed in real-time.
\end{itemize}

\textbf{Sensor Module (Mobile):}
\begin{itemize}
    \item \textbf{FR-06 Data Acquisition:} The mobile application must read data from the accelerometer (linear acceleration) and gyroscope at a frequency of at least 50 Hz.
    \item \textbf{FR-07 Data Transmission:} The mobile app must transmit sensor data to the desktop server via a TCP/IP socket connection (USB tethering).
    \item \textbf{FR-08 Stroke Analysis:} The system must detect the moment of impact (peak acceleration) and calculate the impact force ($F=ma$) and wrist rotation stability.
\end{itemize}

\subsection{Non-functional Requirements}
The non-functional requirements define the quality attributes of the system:

\begin{itemize}
    \item \textbf{NFR-01 Real-time Performance:} The vision processing pipeline must maintain a frame rate of at least 20 FPS (frames per second) to provide smooth visual feedback.
    \item \textbf{NFR-02 Latency:} The latency between the physical cue movement and the AR update should be less than 150 ms to ensure the user perceives the lines as responsive.
    \item \textbf{NFR-03 Accuracy:} The "Ghost Ball" projection error should be less than 5\% of the ball diameter to be practically useful for aiming.
    \item \textbf{NFR-04 Sensor Synchronization:} The time drift between the video impact detection and sensor peak detection must be handled to correctly associate the physical stroke with the visual event.
    \item \textbf{NFR-05 Usability:} The setup process (camera alignment, phone connection) should be performable by a single user within 5 minutes.
\end{itemize}

\section{Use cases}

The interaction between the user (Player) and the system is modeled through several key Use Cases. The primary actor is the Player, who interacts with both the physical equipment (Phone, Cue) and the software (Desktop App).

\begin{figure}[h!]
    \centering
    % Place for your UML Use Case Diagram
    % \includegraphics[width=0.8\textwidth]{figures/use_case_diagram.png}
    \caption{UML Use Case Diagram of the Smart Pool Assistant.}
    \label{fig:use_case_diagram}
\end{figure}

The main use cases are defined as follows:

\begin{enumerate}
    \item \textbf{UC-1 System Setup:} The Player connects the Android phone to the PC (via USB/ADB), starts the server script, and launches the mobile app. The connection is established if the "Connected" status appears.
    \item \textbf{UC-2 Aiming Assistance:} The Player addresses the cue ball. The system detects the cue stick, calculates the trajectory, and projects the "Ghost Ball" and aiming lines. The Player adjusts their stance based on this visual feedback.
    \item \textbf{UC-3 Stroke Execution \& Analysis:} The Player executes the shot. The system automatically detects the impact via the sensor stream, captures a snapshot of the metrics (Force, Rotation), and saves the data to a history log.
    \item \textbf{UC-4 Review History:} The Player views the saved "Hit History" (CSV/Images) to analyze their consistency over the training session.
\end{enumerate}

\section{Description of tools}

The project leverages a modern technology stack combining Computer Vision, AI, and Mobile Development.

\subsection{Hardware Tools}
\begin{itemize}
    \item \textbf{Webcam:} A standard HD webcam is used for video input. It is mounted in a "top-down" configuration to minimize perspective distortion.
    \item \textbf{Android Smartphone:} A device equipped with an Inertial Measurement Unit (IMU). It serves as the telemetry unit attached to the player's cue.
    \item \textbf{Workstation (PC):} A computer with access to the internet and with installed Python 3.x environment to run the vision server and process data.
\end{itemize}

\subsection{Software Tools}
\begin{itemize}
    \item \textbf{Python 3.x:} The core programming language for the backend server and vision processing.
    \item \textbf{OpenCV (Open Source Computer Vision Library):} Used for image pre-processing, drawing the AR visualization (lines, circles), and rendering the real-time telemetry graphs. \cite{bib:opencv_library}
    \item \textbf{Ultralytics YOLO (You Only Look Once):} A state-of-the-art framework for object detection.
    \begin{itemize}
        \item \textit{YOLOv12 (Detection):} Used for robust detection of billiard balls under varying lighting conditions. \cite{bib:yolo12}
        \item \textit{YOLOv8-Pose (Keypoint Estimation):} Specifically used to detect the \texttt{tip} and \texttt{handle} of the cue stick, enabling precise vector calculation. \cite{bib:yolov8_ultralytics}
    \end{itemize}
    \item \textbf{Roboflow:} A platform used for dataset management, image annotation, and versioning. It facilitated the preparation of the training data for the custom billiard model.
    \item \textbf{Android Studio \& Java:} The development environment for the mobile sensor application. Java was chosen for its native support of Android Sensor APIs.
    \item \textbf{ADB (Android Debug Bridge):} Utilized for establishing a low-latency reverse TCP connection (\texttt{adb reverse}) between the Android device and the localhost server.
\end{itemize}

\section{Methodology of design and implementation}

The development followed an iterative **Prototyping Methodology**, which is suitable for systems involving experimental algorithms (like computer vision) and hardware integration. The process was divided into four phases:

\subsection{Phase 1: Data Collection and Model Training}
The initial phase focused on the Vision Module. A custom dataset of billiard balls and cue sticks was collected and annotated using Roboflow. The YOLO models were trained iteratively (using Google Colab GPUs) to achieve high mean Average Precision (mAP) for ball detection and keypoint estimation.

\subsection{Phase 2: Vision Logic Implementation}
Once the models were trained, the Python logic was developed to translate raw detections into game state information. This involved implementing the vector algebra for the "Ghost Ball" algorithm and using OpenCV to render the visual overlays.

\subsection{Phase 3: Sensor Module Development}
The Android application was developed to reliably extract linear acceleration and gyroscope data. The focus was on implementing the `TYPE\_LINEAR\_ACCELERATION` sensor to filter out gravity and establishing a robust TCP socket protocol for data transmission.

\subsection{Phase 4: Integration and Testing}
The final phase involved fusing the two modules. The Python server was updated to handle concurrent threads: one for video processing and one for listening to incoming sensor packets. The system was tested in real-world scenarios to calibrate the impact detection thresholds and verify the synchronization between the visual stroke and the sensor peak.