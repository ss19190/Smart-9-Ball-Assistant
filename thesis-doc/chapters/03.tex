\chapter{Requirements and Tools}

This chapter details the functional and non-functional requirements and presents the technological stack selected for implementation. It also describes the system's use cases and the method adopted for the design and development process.

\section{Functional and Non-functional Requirements}

The system requirements were defined to address the problems identified in the previous chapter, specifically the ``Ghost Ball'' visualization and the biomechanical analysis of the stroke.

\subsection{Functional Requirements}
The functional requirements define the specific behaviors and functions the system must support. They are categorized by a module:

\textbf{Vision Module (Desktop):}
\begin{itemize}
    \item \textbf{FR-01 Video acquisition:} The system must capture a real-time video stream from a webcam positioned above the billiard table.
    \item \textbf{FR-02 Object detection:} The system must detect and classify the cue ball and object balls.
    \item \textbf{FR-03 Pose estimation:} The system must identify keypoints of the cue stick (tip and handle) to determine the aiming vector.
    \item \textbf{FR-04 Trajectory prediction:} The system must calculate the ``Ghost Ball'' position and predict the trajectories of the cue ball and target ball based on geometric rules.
    \item \textbf{FR-05 AR visualization:} The system must overlay the predicted paths and the ghost ball indicator onto the video feed in real-time.
\end{itemize}

\textbf{Sensor Module (Mobile):}
\begin{itemize}
    \item \textbf{FR-06 Data acquisition:} The mobile application must read data from the accelerometer (linear acceleration) and gyroscope at a frequency of at least 50 Hz.
    \item \textbf{FR-07 Data transmission:} The mobile app must transmit sensor data to the desktop server via a TCP/IP socket connection (USB tethering).
    \item \textbf{FR-08 Stroke analysis:} The system must detect the moment of impact (peak acceleration) and calculate the impact force ($F=ma$) and the wrist rotation stability.
    \item \textbf{FR-09 Accuracy:} The ``Ghost Ball'' projection error should be less than 5\% of the ball diameter to be practically useful for aiming.
\end{itemize}

\subsection{Non-functional Requirements}
The non-functional requirements define the quality attributes of the system:

\begin{itemize}
    \item \textbf{NFR-01 Real-time performance:} The vision processing pipeline must maintain a frame rate of at least 20 fps (frames per second) to provide smooth visual feedback.
    \item \textbf{NFR-02 Latency:} The latency between the physical cue movement and the AR update should be less than 150 ms to ensure the user perceives the lines as responsive.
    \item \textbf{NFR-03 Sensor synchronization:} The time drift between the video impact detection and sensor peak detection must be handled to correctly associate the physical stroke with the visual event.
    \item \textbf{NFR-04 Usability:} The setup process (camera alignment, phone connection) should be performable by a single user within 5 minutes.
\end{itemize}

\section{Use Cases}

The interaction between the user (player) and the system is shown by a number of essential use cases. The primary actor is the player, who interacts with both the physical equipment (phone, cue) and the software (desktop app).

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{./graf/usecase.png}
\caption{Use case diagram.}
\label{fig:label}
\end{figure}

The main use cases are defined as follows:

\begin{enumerate}
    \item \textbf{UC-1 System setup:} The player connects the Android phone to the PC (via USB/ADB), starts the server script, and launches the mobile app. The connection is established if the ``connected'' status appears.
    \item \textbf{UC-2 Aiming assistance:} The player addresses the cue ball. The system detects the cue stick, calculates the trajectory, and projects the ``Ghost Ball'' and aiming lines. The player adjusts their stance based on this visual feedback.
    \item \textbf{UC-3 Stroke execution \& analysis:} The player executes the shot. The system automatically detects the impact via the sensor stream, captures a snapshot of the metrics (force, rotation), and saves the data to a history log.
    \item \textbf{UC-4 Review history:} The player views the saved ``hit history'' (csv/images) to analyze their consistency over the training session.
\end{enumerate}

\section{Description of Tools}
The project applies a modern technology stack combining computer vision, AI (Artificial Intelligence), and mobile development.

\subsection{Hardware Tools}
\begin{itemize}
    \item \textbf{Webcam:} A standard HD webcam is used for video input. It is mounted in a ``top-down" configuration to minimize perspective distortion.
    \item \textbf{Android smartphone:} A device equipped with an IMU. It works as the telemetry unit attached to the player's cue.
    \item \textbf{Workstation (PC):} A computer with access to the internet and with installed Python 3.x environment to run the vision server and process data.
\end{itemize}

\subsection{Software Tools}
\begin{itemize}
    \item \textbf{Python 3.x:} The main programming language for the backend server and vision processing~\cite{bib:python}.
    \item \textbf{OpenCV (open source computer vision library):} Used for image pre-processing, drawing the AR visualization (lines, circles), and rendering the live telemetry graphs~\cite{bib:opencv-library}.
    \item \textbf{Ultralytics YOLO (You Only Look Once):} A state-of-the-art framework for object detection.
    \begin{itemize}
        \item \textit{yolov12 (detection):} It is used for robust detection of billiard balls under different lighting conditions~\cite{bib:yolo12}.
        \item \textit{yolov8-pose (keypoint estimation):} It is specifically used to detect the \texttt{tip} and \texttt{handle} of the cue stick, allowing accurate vector calculation~\cite{bib:yolov8-ultralytics}.
    \end{itemize}
    \item \textbf{Roboflow:} A platform used for dataset management, image annotation, and versioning. It facilitated the preparation of the training data for the custom billiard model~\cite{bib:roboflow}.
    \item \textbf{Android Studio \& Java:} The development environment for the mobile sensor application. Java was chosen for its native support of Android sensor APIs~\cite{bib:android}.
    \item \textbf{ADB (Android Debug Bridge):} It is utilized for establishing a low-latency reverse TCP connection (\texttt{adb reverse}) between the Android device and the localhost server~\cite{bib:adb}.
\end{itemize}

\section{Methodology of Design and Implementation}

The development followed an iterative prototyping methodology~\cite{bib:method}, which is suitable for systems that include experimental algorithms (like computer vision) and hardware integration. The process was divided into four phases:

\subsection{Phase 1: Data Collection and Model Training}
The initial phase focused on the preparation of the Computer Vision module. Two separate custom datasets were curated and annotated using the Roboflow platform~\cite{bib:roboflow}: one for billiard balls and one for the cue stick.

For the \textbf{Ball Detection model}, a dataset of overhead table images was collected. As shown in the dataset analysis (see Figure~\ref{fig:labels_ball}), the data reflects the natural logic of the game. Specifically, chart (a) highlights the significant class imbalance: the ``other" class (object balls 1--15) is approximately 15 times more frequent than the ``cue ball" class. The bounding box analysis (b) and spatial distribution (c) confirm that balls are clustered within the table area, while the size correlation (d) indicates consistent object dimensions across the dataset.

To reduce possible overfitting as well as improve model dependability under different lighting conditions, image augmentation procedures (including rotation, brightness adjustment, and noise augmentation) were applied during the preprocessing stage.

A similar process was followed for the \textbf{Cue Detection model}, where the dataset was annotated with keypoints (skeleton) to define the tip and handle positions for the pose estimation task. The class distribution for this dataset (see Figure~\ref{fig:labels_cue}) is uniform as it focuses on a single class, ``cue".

\subsection{Phase 2: Vision Logic Implementation}
Once the models were trained, the Python logic was developed to translate raw detections into the game state information. This involved implementing the vector algebra for the ``Ghost Ball''~\cite{bib:ghost-ball} algorithm and using OpenCV~\cite{bib:opencv-library} to render the visual overlays.

\subsection{Phase 3: Sensor Module Development}
The Android application was developed to reliably extract linear acceleration and gyroscope data. The focus was on implementing the \texttt{TYPE\_LINEAR\_ACCELERATION}~\cite{bib:linear} sensor to filter out gravity, ensuring that only the player's dynamic force is measured. 

Furthermore, a stable TCP socket protocol was established for data transmission. This phase required the sensor sampling rate optimizing to capture rapid stroke movements and implementing a JSON-based serialization scheme. 

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{./graf/ball-detection/labels - ball.jpg}
    \caption{Analysis of the ball detection dataset: (a) Class distribution showing the massive disproportion between ``other" balls and the single ``cue ball"; (b) Bounding box density map; (c) Spatial distribution of object centers; (d) Correlation of bounding box width and height.}
    \label{fig:labels_ball}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{./graf/cue-detection/labels.jpg}
    \caption{Analysis of the cue detection dataset: (a) Single-class distribution; (b) Bounding box density map; (c) Spatial distribution of cue centers; (d) Correlation of bounding box width and height.}
    \label{fig:labels_cue}
\end{figure}